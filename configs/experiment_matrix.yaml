# ============================================================
# EXPERIMENT MATRIX — FROZEN 2026-02-13
# Do NOT modify after first results are committed.
# ============================================================
experiment:
  name: maia-inspired-phi35-h100
  date_frozen: "2026-02-13"
  model: microsoft/Phi-3.5-mini-instruct
  model_revision: main  # pin to commit hash once validated

# ---- Fairness constraints (held constant across ALL configs) ----
fairness_constraints:
  tokenizer: same HF tokenizer (use_fast=True, trust_remote_code=True)
  chat_template: NOT applied for standard completion tasks (hellaswag/arc/lambada)
  decoding:
    temperature: 0.0
    do_sample: false
    top_p: 1.0
    seed: 42
  runtime: torch_runner for all ablation configs (C0-C5)
  warmup_runs: 1  # discarded before timing
  kv_cache_dtype: auto  # matches weight dtype per config

# ---- Configuration matrix ----
configs:
  C0:
    name: BF16 baseline
    runtime: torch
    quant:
      mode: none
      dtype: bf16

  C1:
    name: FP8 activations (optional)
    runtime: torch
    quant:
      mode: fp8_te
      dtype: bf16
    notes: "Requires TransformerEngine; skip if unavailable on Colab image"

  C2:
    name: W4A16 — industry baseline (bitsandbytes NF4)
    runtime: torch
    quant:
      mode: w4a16_bnb
      dtype: bf16
    notes: "bitsandbytes NF4, double-quant enabled"

  C3:
    name: W4 exact-scale (SMQ reference, scale_mbits=-1)
    runtime: torch
    quant:
      mode: w4_shared_scale
      scale_mbits: -1   # exact FP32 scales — SMQ ablation reference
      group_size: 128
      dtype: bf16
      quant_target: mlp
    notes: "Same arch as C4/C5 but with exact per-group scales. Isolates SMQ effect."

  C4:
    name: W4 + SMQ scale_mbits=5 (proposed method)
    runtime: torch
    quant:
      mode: w4_shared_scale
      scale_mbits: 5
      group_size: 128
      dtype: bf16
      quant_target: mlp

  C5:
    name: W4 + SMQ ablation (scale_mbits sweep)
    runtime: torch
    quant:
      mode: w4_shared_scale
      scale_mbits_sweep: [0, 3, 5]
      group_size: 128
      dtype: bf16
      quant_target: mlp
    notes: "Run as three separate runs with scale_mbits=0, 3, 5"

workloads:
  offline_batch_decode:
    prompt_lengths: [128, 512, 2048]
    output_lengths: [64, 256]
    repeats: 3

  online_serving:
    concurrency: [1, 4, 8, 16, 32]
    prompt_len: 512
    output_len: 128
    num_prompts: 64
    runner: vllm  # separate from torch_runner ablation

  long_context_stress:
    prompts: [8192, 32768]
    output_len: 64
    enabled_if_memory_allows: true

metrics:
  performance:
    - throughput_output_tok_s
    - throughput_total_tok_s
    - ttft_ms
    - tpot_ms
    - peak_gpu_mem_mb
    - joules_per_token   # pynvml trapezoidal integration; flag if sampler unavailable

  quality:
    harness: lm-eval
    tasks: [hellaswag, arc_challenge, gsm8k, lambada_openai]
    num_fewshot: 0       # zero-shot for instruct model
    report_relative_drop_vs: C0
    acceptance_target_avg_relative_drop_pct: 3.0
